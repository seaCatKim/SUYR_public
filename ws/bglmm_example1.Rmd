---
title: "Bayesian GLMM Part1"
author: "Murray Logan"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: show
    collapse: no
    df_print: paged
    fig_caption: yes
    fig_height: 4
    fig_width: 4
    highlight: textmate
    theme: spacelab
    toc: yes
    toc_float: yes
    css: ../resources/style.css
  pdf_document:
    df_print: default
    fig_caption: yes
    fig_height: 4
    fig_width: 4
    highlight: tango
    latex_engine: xelatex
    number_sections: yes
    toc_depth: 2
  word_document:
    fig_caption: yes
    fig_height: 4
    fig_width: 4
    highlight: tango
    toc: yes
    toc_depth: 2
output_dir: "docs"
documentclass: article
fontsize: 12pt
mainfont: Arial
mathfont: LiberationMono
monofont: DejaVu Sans Mono
classoption: a4paper
bibliography: ../resources/references.bib
---

```{r setup, include=FALSE, warnings=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```

# Preparations

Load the necessary libraries

```{r libraries, results='markdown', eval=TRUE, message=FALSE, warning=FALSE}
library(rstanarm)   #for fitting models in STAN
library(brms)       #for fitting models in STAN
library(coda)       #for diagnostics
library(bayesplot)  #for diagnostics
library(ggmcmc)     #for MCMC diagnostics
library(rstan)      #for interfacing with STAN
library(emmeans)    #for marginal means etc
library(broom)      #for tidying outputs
library(DHARMa)     #for residual diagnostics
library(tidybayes)  #for more tidying outputs
library(ggeffects)  #for partial plots
library(broom.mixed)
library(tidyverse)  #for data wrangling etc
```

# Scenario

A plant pathologist wanted to examine the effects of two different strengths of tobacco virus on the number of lesions on tobacco leaves.  She knew from pilot studies that leaves were inherently very variable in response to the virus.  In an attempt to account for this leaf to leaf variability, both treatments were applied to each leaf.  Eight individual leaves were divided in half, with half of each leaf inoculated with weak strength virus and the other half inoculated with strong virus.  So the leaves were blocks and each treatment was represented once in each block.  A completely randomised design would have had 16 leaves, with 8 whole leaves randomly allocated to each treatment.  

![Tobacco plant](../resources/TobaccoPlant.jpg){height="300"}

Format of tobacco.csv data files

LEAF   TREAT    NUMBER
------ -------- --------
1      Strong   35.898
1      Week     25.02
2      Strong   34.118
2      Week     23.167
3      Strong   35.702
3      Week     24.122
\...   \...     \...

------------ ----------------------------------------------------------------------------------------------------
**LEAF**     The blocking factor - Factor B
**TREAT**    Categorical representation of the strength of the tobacco virus - main factor of interest Factor A
**NUMBER**   Number of lesions on that part of the tobacco leaf - response variable
------------ ----------------------------------------------------------------------------------------------------


# Read in the data

Should be counts but averaged counts from subsamples on leaf. Often ok to model with Gaussian - central limit theorm - if numbers are big enough and far enough from 0.

```{r readData, results='markdown', eval=TRUE}
tobacco = read_csv('../data/tobacco.csv', trim_ws=TRUE)
glimpse(tobacco)
```

```{r factorize}
tobacco <- tobacco %>% mutate(LEAF = factor(LEAF),
                              TREATMENT = factor(TREATMENT))
tobacco %>% head()
```


# Exploratory data analysis


Model formula:
$$
y_i \sim{} \mathcal{N}(\mu_i, \sigma^2)\\
\mu_i =\bf{X_i}\boldsymbol{\beta} + \bf{Z_i}\boldsymbol{\gamma}\\
\beta_0 \sim{} \mathcal{N}(31, 10)\\
\beta_1 \sim{} \mathcal{N}(0, 10)\\
\boldsymbol{\gamma} \sim{} \mathcal{N}(0, \boldsymbol{\Sigma})\\
\boldsymbol{\Sigma} = \boldsymbol{D}({\sigma_l})\boldsymbol{\Omega}\boldsymbol{D}({\sigma_l})\\
\boldsymbol{\Omega} \sim{} LKJ(\zeta)\\
\sigma_j^2 \sim{} \Gamma(1,1)\\
\sigma^2 \sim{} \mathcal{Cauchy}(0,2)
$$

where:

- $\bf{X}$ is the  model matrix representing the overall intercept and effects of the treatment on the number of lesions.
- $\boldsymbol{\beta}$ is a vector of the population-level effects parameters to
be estimated.
- $\boldsymbol{\gamma}$ is a vector of the group-level effect parameters
- $\bf{Z}$ represents a cell means model matrix for the random intercepts (and
  possibly random slopes) associated with leaves.
- the population-level intercept ($\beta_0$) has a gaussian prior with location
  of 31 and scale of 10
- the population-level effect ($\beta_1$) has a gaussian prior with location of
  0 and scale of 10
- the group-level effects are assumed to sum-to-zero and be drawn from a
  gaussian distribution with mean of 0 and covariance of $\Sigma$  
- $\boldsymbol{\Sigma}$ is the variance-covariance matrix between the groups
(individual leaves).  It turns out that it is difficult to apply a prior on this
covariance matrix, so instead, the covariance matrix is decomposed into a
correlation matrix ($\boldsymbol{\Omega}$) and a vector of variances
($\boldsymbol{\sigma_l}$) which are the diagonals ($\boldsymbol{D}$) of the
covariance matrix.

$\beta_0$ is the average number of lesions for the strong group.
$\beta_1X_1$ slope is the averabe of all the slopes.
$\gamma_z$ leaf variance?

- $\boldsymbol{\Omega}$ 
$$
\gamma \sim{} N(0,\Sigma)\\
\Sigma -> \Omega, \tau\\
$$
where $\Sigma$ is a covariance matrix.

It turns out that it is difficult to apply a prior on a covariance matrix, so
instead, we decompose the covariance matrix into a correlation matrix and variance.

- Covariance matrix can be decomposed into a correlation matrix and a vector of
  variances
- The variances can be further decomposed into the product of a simplex vector
  (which is a probability vector, non-negative and sums to 1)
  and the trace (product of the order of the matrix and the scale of the scale
  parameter, also the sum of its diagonal elements) of a matrix.  Each element of the simplex vector represents the
  proportion of the trace that is attributable to the corresponding variable.
- A prior on all the above is a decov (decomposition of covariance) function

- The prior on the correlation matrix is called LKJ
- density is proportional to the determinant of the correlation matrix raised to
  the power of the positive regularization paramter minus one.

- The prior on the simplex vector is a symmetric Dirichlet prior which has a
single (positive) concentration parameter (default of 1 implying the prior is
jointly uniform over the same of simplex vectors of that size)
A symmetric Dirichlet prior is used for the simplex vector.  The Dirichlet
  prior has a single (positive) concentration parameter

- The positive scale paramter has a gamma prior (with default shape and scale of
  1 - implying a unit-exponential distribution)

- alternatively, the lkj prior can be used for covariance.
- as with decov, it decomposes into correlation and variances, however the
  variances are not further decomosed into a simplex vector and trace.
- instead the standard deviations (variance squared) for each of the group
  specific paramters are given half student-t distribution with scale and df
  paramters specified through the scale (default 10) and df (default 1)
  arguments of the lkj function.
- the lkj prior is similar, yet faster than decov

# Priors

```{r medians mads}
tobacco %>% group_by(TREATMENT) %>% 
  summarize(median(NUMBER),
            mad(NUMBER))
```

Effects always centered on 0, some could be positive, some negative.

10 variance for $\beta_1$ rough difference between levels.

Variance for leaves could be very small - all leaves identical - and so want distribution with mass close to 0.

$$
\beta_0 \sim{} \mathcal{N}(35, 5)\\
\beta_1 \sim{} \mathcal{N}(0, 10)\\
\sigma^2 \sim{} \Gamma(2,0.5)\\
\sigma_j^2 \sim{} \mathcal{Cauchy}(0,2)
$$

Numbers are large ~30s so gamma(2,1) might be too narrow. Very little variation around 5. Want the gamma a bit wider.

Cauchy has mass close to 0 which is what we want for random effects. Cauchy has shrinkage tendency. Likely to have small variance, the more leaves you average variance is likely to be small.

```{r viz gamma}
standist::visualize("gamma(2,1)", xlim = c(-10, 50))
standist::visualize('gamma(2,1)', 
"gamma(2,0.5)",
"cauchy(0,5)", # 0 is always 0, go up in 1, 2, 5, 10 for 2nd number - want 'squared squares'
"cauchy(0,2)",
xlim = c(-10, 50))
```

```{r set prior}
priors <- prior(normal(35,5), class = 'Intercept') +
  prior(normal(0, 10), class = 'b') +
  prior(gamma(2, 0.5), class = 'sigma') + # sigma is family variance
  prior(cauchy(0, 5), class = 'sd') # sd is the random effect
```

# Fit the model {.tabset .tabset-faded}

Forumla resembles formula for frequentist random effects.

(1|LEAF) - random intercept for each leaf.

```{r formula}
tobacco.form <- bf(NUMBER ~ (1|LEAF) + TREATMENT,
                   family = gaussian())
```

## Prior only

```{r fit model, cache  = T}
tobacco.brm2 <- brm(tobacco.form,
                    data = tobacco,
                    iter = 5000,
                    warmup = 2500,
                    prior = priors,
                    sample_prior = 'only',
                    chains = 3,
                    thin = 5)
```

Bit suspect. Expect range to be wider - 100s to 0. Could make 

```{r conditional plot of priors}
tobacco.brm2 %>% conditional_effects()
```

## Prior and data model

```{r update model, cache = T}
tobacco.brm2<- update(tobacco.brm2, sample_prior = 'yes')
```

```{r prior and data conditional}
tobacco.brm2 %>% conditional_effects()
```

# Random intercept and slope

TREATMENT|LEAF - treatment varies within a leaf. 

```{r random slope formula}
tobacco.form <- bf(NUMBER ~ (TREATMENT|LEAF) + TREATMENT,
                   family = gaussian())
```

**Warning:** Divergent transitions after warmup. Divergent transition is like the ball being hit and the ball falling off the surface. Has to try and start again like golf hitting back into the water and going back to whereyou started. Also likely to hit it off again.

Random effects model often have a 'funnel' shape. In wider bit easy to sample but traveling down to skinny bit and falling off.

One or two is okay (under 10) - but hundreds need to deal with it. Any are considered not ideal.

1. Could tighten standard deviation priors.
2. Improve the learning of figuring out step length - can take model longer to run - similar to taking small steps. The velocity the ball on surface is being hit.

If still have divergent transitions with adapt_delta could go back and modify Cauchy - decrease 5 to 2.

```{r fit random slope model, cache = TRUE}
tobacco.brm3 <- brm(tobacco.form,
                    data = tobacco,
                    prior = priors,
                    sample_prior = 'yes',
                    iter = 5000,
                    warmup = 2500,
                    chains = 3,
                    thin = 5,
                    control = list(adapt_delta = 0.9)) # default is 0.7, uses great percentage of warmup to attempt to stabilize learning the velocity
```

## Compare the random intercept/slope model

leave one out

Could get a warning x observations with a pareto_k > 0.7 - typically because of small dataset. Measure of whether you can simulate each observation because values are unusual so might not be simulating very well. If have lots cannot rely on information criteria. One or ok - few 'bad' okay.

Slope model is better but by little - SEs overlap.

```{r loo}
(l.1 <- tobacco.brm2 %>% loo()) # wrap in () assign and print
(l.2 <- tobacco.brm3 %>% loo())
```

Top model is better and shows different between two.

Difference is not much - likely not much difference in the results. In Bayesian, do not take up degrees of freedom having more complicated model.

```{r loo compare}
loo_compare(l.1 = l.1, l.2)
```

Priors that we used. 

We set Intercept, b, Cauchy (sd), gamma (sigma).

With random intercept and slope, model also includes correlation between intercept and slope. If allow both the vary they are likely to be correlated.

Have a line will pivot around a multidimensional mean of x and y -> correlation between intercept and slope.

Buried deep within variance-covariance matrix. Defaults for those is the only sensible prior could only apply - don't need to worry about it. Is a 1 - correlation should be between -1 and 1. Don't know/don't care if correlation is negative or prior. Default very unlikely to change.

One for overall model and one for random effects - one for everything is fine. 

```{r prior summary}
prior_summary(tobacco.brm3)
```

b_ is posteriors, sd_ variability between leaves, r_ difference between intercept and slopes.

```{r get model variables}
tobacco.brm3 %>% get_variables()
```




Prior much wider than posterior, don't even have the same center - so good.

This one and also plot fo sd_ s (2 more).

```{r plot priors and posterior}
tobacco.brm3 %>% hypothesis('TREATMENTWeak = 0') %>% plot()
tobacco.brm3 %>% hypothesis('LEAF__Intercept = 0', class = 'sd') %>% plot()
tobacco.brm3 %>% hypothesis('LEAF__TREATMENTWeak = 0', class = 'sd') %>% plot()
# tobacco.brm3 %>% hypothesis('sigma', class = 'sigma') %>% plot()
```

# MCMC Validation {.tabset .tabset-faded}

Select variables to test.

```{r regex variables}
params <- tobacco.brm3 %>% get_variables()
wch <- grepl('^b_.*|^sd_.*|^cor_.*|^sigma$', params, perl = TRUE) # in quotes for grep
wch
params1 <- params[wch]
params1
```

## Trace plots

```{r trace}
tobacco.brm3$fit %>% stan_trace(pars = params1)

```

## Autocorrelation

Some suggesting low level autocorrelation in sigma. Slowing down learning rate making shorter step to autocorrelation - only 1 variable here. Solution is to increase thinning to +4, where it dropped off to nothing, so thin to 10. Thinning keeps every Xth. If upping the thinning would also increase iterations. Can start with 10-20k so wouldn't have to increase iterations when upping thinning.

```{r ac}
tobacco.brm3$fit %>% stan_ac(pars = params1)
```

## Rhat

All good - less than 1.05.

```{r rhat}
tobacco.brm3$fit %>% stan_rhat()
```

## Efficiency

```{r ESS}
tobacco.brm3$fit %>% stan_ess()
```

## Spaghetti

Match distribution of actual data to realizations. With small datasets might look strange.

```{r spaghetti}
tobacco.brm3 %>% pp_check(type = 'dens_overlay', nsamples = 250)
```

# Model validation

DHARMa residuals.

```{r DHARMa}
preds <- tobacco.brm3 %>% posterior_predict(nmsaples = 250, summary = FALSE)
tobacco.resids <- createDHARMa(simulatedResponse = t(preds),
                               observedResponse = tobacco$NUMBER,
                               fittedPredictedResponse = apply(preds, 2, median),
                               integerResponse = FALSE)
plot(tobacco.resids, quantreg = FALSE)
```

Underdispersed - red line is to the left. Will make test more conservatives which is not a bad thing - if still have effects with underdispersed model would leave it.

```{r dispersion}
testDispersion(tobacco.resids)
```

# Partial plot

Expecting:

- negative effect of weak

```{r partial plot}
tobacco.brm3 %>% 
  conditional_effects() %>% 
  plot(points = TRUE)
```

# Model investigation

Population level effects like fixed effects:

- Intercept (mean of 1st group) - 35. 
- Negative effect of treatment between strong-weak. 
- Credibility invervals do not include 0 so at a 95% interval there is strong evidence of an effect.

Family specific parameters - sigma for y.

- Estimated around 4. About 4 units of variation within a treatment within a leaf. 
- How variable treatments withn leaves are - how variable strong is within leaves.

Group level or random effects:

- Consistency of treatment effect - some leaves don't act the same.
- sd(Intercept) - gamma sigma - how varied are the intercepts are at Strong treatment. Fairly similar to sigma ~4. Lesions vary homogenously over those scales both between leaves and within leaves (all for just strong treatment)
- sd(TREATEMENTWeak) - treatment effect between leaves.

More implcations of futher work. Here is okay. Variability between leaf and treatment similar.

Could look at variability between transect/site/reef and get an idea if need more/less of experimental design.

How things vary on different scales.

```{r summary}
tobacco.brm3 %>% summary()
```

## R2

How much does treatment (fixed effects) account for the variability. Around 35%. 

Marginalizing or averaging over the random effect

```{r marginal random effect}
tobacco.brm3 %>% 
  bayes_R2(re.form = NA, summary = FALSE) %>% 
  median_hdci()
```

Conditional on just the random intercepts. 61% 

```{r conditional}
tobacco.brm3 %>% 
  bayes_R2(re.form = ~(1|LEAF), summary = FALSE) %>% 
  median_hdci()
```

Take into account leaves respond differently to treatment - slopes and intercept - 68%.

```{r intercet and slope}
tobacco.brm3 %>% 
  bayes_R2(re.form = ~(TREATMENT|LEAF), summary = FALSE) %>% 
  median_hdci()
```

