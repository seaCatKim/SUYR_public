---
title: "GLM Part2"
author: "Murray Logan"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: show
    collapse: no
    df_print: paged
    fig_caption: yes
    fig_height: 4
    fig_width: 4
    highlight: textmate
    theme: spacelab
    toc: yes
    toc_float: yes
    css: ../resources/style.css
  pdf_document:
    df_print: default
    fig_caption: yes
    fig_height: 4
    fig_width: 4
    highlight: tango
    latex_engine: xelatex
    number_sections: yes
    toc_depth: 2
  word_document:
    fig_caption: yes
    fig_height: 4
    fig_width: 4
    highlight: tango
    toc: yes
    toc_depth: 2
output_dir: "docs"
documentclass: article
fontsize: 12pt
mainfont: Arial
mathfont: LiberationMono
monofont: DejaVu Sans Mono
classoption: a4paper
bibliography: ../resources/references.bib
---
 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE,cache.lazy = FALSE, tidy='styler', 
                      fig.width = 10, fig.height = 10)
```

# Preparations

Load the necessary libraries

```{r libraries, results='markdown', eval=TRUE, message=FALSE, warning=FALSE}
library(car)       #for regression diagnostics
library(broom)     #for tidy output
library(ggfortify) #for model diagnostics
library(sjPlot)    #for outputs
library(knitr)     #for kable
library(effects)   #for partial effects plots
library(emmeans)   #for estimating marginal means
library(ggeffects) #for partial effects plots
library(modelr)    #for auxillary modelling functions
library(DHARMa)    #for residual diagnostics plots
library(performance) #for residuals diagnostics
library(see)         #for plotting residuals
library(tidyverse) #for data wrangling
```

# Scenario

@Polis-1998-490 were interested in modelling the presence/absence of lizards
(<i>Uta sp.</i>) against the perimeter to area ratio of 19 islands in the Gulf
of California.

![Uta lizard](../resources/uta.jpg){width="200" height="137"}

Format of polis.csv data file

ISLAND       RATIO   PA
------------ ------- ----
Bota         15.41   1
Cabeza       5.63    1
Cerraja      25.92   1
Coronadito   15.17   0
..           ..      ..

------------ -----------------------------------------------------------------------------------------
**ISLAND**   Categorical listing of the name of the 19 islands used - variable not used in analysis.
**RATIO**    Ratio of perimeter to area of the island.
**PA**       Presence (1) or absence (0) of *Uta* lizards on island.
------------ -----------------------------------------------------------------------------------------

The aim of the analysis is to investigate the relationship between island
perimeter to area ratio and the presence/absence of Uta lizards.

# Read in the data

```{r readData, results='markdown', eval=TRUE}
polis = read_csv('../data/polis.csv', trim_ws=TRUE)
```

```{r examinData}
glimpse(polis)
head(polis)
str(polis)
summary(polis)
```

Response: PA - presence/absence
Predictor: Ratio - island perimeter to area
Bernoulli - special case of binomial

For number of female joeys:
dataframe would be 'RATIO, FEMALE, TOTAL-FEMALE'
glm(cbind(Females, Total-Female) ~ formula)

Need the positives and the negatives.

$$
PA_i \sim Bin(p_i, 1) \\
log\frac{p_1}{1-p_i} = \beta_0 + \beta_1 x
$$
 
# Exploratory data analysis

Model formula:
$$
y_i \sim{} \mathcal{Bin}(n, p_i)\\
ln\left(\frac{p_i}{1-p_i}\right) = \beta_0 + \beta_1 x_i
$$

where $y_i$ represents the $i$ observed values, $n$ represents the number of
trials (in the case of logistic, this is always 1), $p_i$ represents the
probability of lizards being present in the $i^{th}$ population, and $\beta_0$
and $\beta_1$ represent the intercept and slope respectively.

```{r scatterplot}
ggplot(polis, aes(x = RATIO, y = PA)) +
  geom_point()
```


# Fit the model {.tabset .tabset-faded}

```{r fit a binomial glm}
polis.glm <- glm(PA ~ RATIO, family = binomial(link='logit'), data = polis)
```


# Model validation {.tabset .tabset-faded}

Model validation routines draw heavily upon patterns in residuals.
For OLS models, the residuals are fairly straight forward.  They are
the difference between the observed and predicted and are the property
that are minimised during optimisation (least squares refers to
minimising the sum of square residuals).

This is not the case for models based on maximum likelihood.  Rather
than minimising the sum of residuals, GLM's maximise likelihood.
Hence residuals are not directly calculated as part of the
optimisation process.  Nevertheless, residual patterns are useful
diagnostics.

For GLM's there are numerous different residual formulations:

| Residual type | Description                                                 | Function                     |
|---------------|-------------------------------------------------------------|------------------------------|
| Working       | Residuals on the link scale transformed back to link scale.  | `residuals(mod, "working")`  |
|               | `(y - mu)/mu.eta(eta)`                                      |                              |
| Deviance      | Signed square-root of the deviance due to each observation. | `residuals(mod, "deviance")` |
|               | The sum of the deviance residuals is the total deviance.    |                              |
| Pearson       | The difference between observed and expected scaled by the  | `residuals(mod, "pearson")`  |
|               | standard deviation so they are on the response scale        |                              |
| Partial       | Working residuals added to the fitted values. Vector per    | `residuals(mod, "partial")`  |
|               | Predictor                                                   |                              |
| Response      | Raw residuals - difference between observed and expected    | `residuals(mod, "response")` |
|               | These are only appropriate for Gaussian                     |                              |

Standarized residuals from plot often *Pearson* residuals - resid/stdev.

Partial residuals - residuals for a single variable holding all other variables constant

Reponse - raw residuals - difference between observed and expected. Only appropriate for Gaussian.

OLS - observed data (yi) trying to minimize residuals for prediction. Calculated *in* the model.
Working residuals (glm) - can be calculated on any model. yi -> prediction -> residuals

These two should be the same for a guassian model.

## autoplot

```{r autoplot}
polis.glm %>% autoplot(which = 1:6, label.repel = TRUE)
```

Because logistic regression - residuals don't mean anything. Why DHARMa exists.
Q-Q looks good.
Cook's - row 3 more influential than others > 0.8

One grain of doubt model might be okay. Good to get more information about row 3. 

```{r mutate to get row 3 and plot}
polis %>% mutate(n = 1:n()) %>%  # alt to n() can use nrow(.), '.' is dataset that piping sent
  ggplot(aes(x = RATIO, y = PA)) +
  geom_text(aes(label = n))
```

Would expect ratio at 3 would not have the lizards. Decide if we want to remove or not. Adds more uncertainty. Also, makes it anti-conservative as in if the effect is still detected with this point shows the strength of the effect.

## check_model

```{r check_model}
polis.glm %>% performance::check_model()
```

##DHARMa

```{r DHARMa}
polis.resid <- polis.glm %>% 
  simulateResiduals(plot = TRUE)
```

Conclude no evidence there is an issue that would invalidate our model.

**Reminder** should make a statement that would put into the methods.

# Partial plots {.tabset .tabset-faded}

There are also numerous routines and packages to support these fitted trends
(partial plots).

| Package     | Function                   | Type              | Notes             |
|-------------|----------------------------|-------------------|-------------------|
| `sjPlot`    | `plot_model(type = 'eff')` | Marginal means    |                   |
| `effects`   | `allEffects()`             | Marginal means    |                   |
| `ggeffects` | `ggeffects()`              | Marginal means    | calls `effects()` |
| `ggeffects` | `ggpredict()`              | Conditional means | calls `predict()` |
| `ggeffects` | `ggemmeans()`              | Marginal means    | calls `emmeans()` |
|             |                            |                   |                   |

## plot model

Bit jagged, not using 100 points like previous example. 

Raw data is 0, 1s (presence/absence). Showing a back-transformation of the model of probability. Probability of lizards being present. As ratio decreases the ratio declines in a sigmoidal shape - happens when straight line is back-transformed. Fit a straight on our link/logit scale. Modeled not PA, but PA through logit function through binomial distribution.

```{r plot model validation}
polis.glm %>%  plot_model(type = 'eff', show.data = TRUE)
```

## allEffects

In effects package for generating partial effects. `effects()` will give you plot of one effect. Uses base plot graphics.

Blue modelled line is a straight line. There is an intercept and a slope.

Pink is loess smoother on raw data.

Pink circles is supposedly the data, but only had 0, 1s. 'residuals = TRUE' takes predicted value and adds residuals - here not using the correct residuals (see table) so ignore here. Reasonably good for anything but binomials.

```{r allEffects}
polis.glm %>% allEffects(residuals = TRUE) %>% 
  plot()

polis.glm %>% allEffects(residuals = TRUE) %>% 
  plot(type = 'response') # transforms everything back on response scale, 0-1 here
```

## ggpredict()

```{r ggpredict}
polis.glm %>% ggpredict() %>% 
  plot(add.date = TRUE, jitter = FALSE)
```

## emmeans

```{r emmeans}
polis.glm %>% ggemmeans(~RATIO) %>% 
  plot(add.data = TRUE, jitter = FALSE)
```

All plots are similiar and should be.

Expect to see some sort of negative relationship. Probability of lizards decreases as ratio increases.

# Model investigation / hypothesis testing {.tabset .tabset-faded}

Have an intercept of 3.6 which is on the log odds ratio scale.

Odds ratio:

Probatility of the lizard being present over probability of lizard being absent.

$\log(\frac{p_i} {1-p_i}) = \beta_0 + \beta_1 x_i$

Odds of something happenening compared to something that is not. 

## Model summary

```{r summary}
polis.glm %>% summary()
```

## Back-transform intercept

Lizard is 36x more likely to be present at low ratios.

$p_i / 1-p_i$ -> $p_i$

```{r backtransform from log-odds ratio}
3.606 - 0.2196
exp(3.606 - 0.2196) # log laws subtraction turs to division etc.

exp(3.606) # this is log-odds back to odds ratio
plogis(3.606)
```

97% chance you will find lizards on an island with a very small (0 ratio) island.

Function staring with 'r' will give random draws for x distribution (t, norm, etc.)

Function staring with 'p' will give probability x distribution (t, norm, etc.)

```{r example }
rt(10, 5)
pt(10, 5)
```

Intercept hypothesis not that useful.

## RATIO estimates

RATIO test is significant.
Is a rate - -0.2196 decline on a **link scale**.

```{r exp RATIO estimate}
exp(-0.2196)

polis.glm %>% coef %>% exp

36.8 * 0.8 # one change in island ratio unit
```

## Stats summary

When island ratio is 0, lizards are 36x more likely to be present.

For every one unit change in the island shape, the odds of lizards being present declines by 20% (1-0.8). Eventually, odds of lizards being present will diminish to close to 0. 

## tidy CI

```{r CI}
polis.glm %>% tidy(conf.int = TRUE)

polis.glm %>% tidy(conf.int = TRUE, exponentiate = TRUE) %>% # on response scale
  knitr::kable()
```

# Predictions {.tabset .tabset-faded}

## $R^2$

In simple regression, the $R^{2}$ value (coefficient of determination)
is interpreted as the amount of variation in the response that can be
explained by its relationship with the predictor(s) and is calculated
as the sum of squares explained divided by the sum of squares total.
It is considered a measure of the strength of a relationship ans is
calculated as:

$$
R^2 = 1 - \frac{\sum_i=1^N (y_i - \hat(y_i)^2)}{\sum_i=1^N (y_i - \bar(y))^2}
$$
where $y_{i}$ and $\hat{y_{i}}$ are the $i^{th}$ observed and predicted value respectively, $\bar{y}$ is the mean of the observed values and $N$ is the total number of observations.

This is really only appropriate for OLS.  For other models there are alternative measures (**psuedo R-squared**) that can be appled depending on how they are to be interpreted:

1. **Explained variance**.  If we consider the numerator as a measure of the unexplained variance and the denominator as a measure of the total variance, then their one minus the ratio is the proportion of the variance explained by the model.
2. **Improvement of a fitted model over a null model**.  The numerator is a measure of the variance unexplained after fitting the model and the denominator is the amount of variance unexplained after fitting a null model (model with only an intercept - essentially just the response mean).  The ratio therefore reflects the improvement. 
3. **Square of correlation**. The squared correlation between the predicted and observed values.

These will be the same for OLS. For GLM's, need to decide how you are interpreting and will inform how you calculate it (see below table).

There are many different ways to calculate $R^2$ values from GLM's.  The
following table provides some guidance as to which method is appropriate for which type of model and how they should be interpreted..

One very simple calculation is based on deviance (a measure of the
total amount unexplained) as:

$$
1-\frac{Deviance}{Deviance_{NULL}}
$$

where $Deviance_{NULL}$ is the deviance of a null model (e.g. `glm(PA ~ 1, data=polis, family='binomial')`)

Alternatively, there are many different ways to calculate $R^2$ values
from GLM's.  The following table provides some guidance as to which
method is appropriate for which type of model and how they should be
interpreted..

| Model             | Appropriate $R^2$ | Formula                         | Interpreted as | Function                        |
| ----------------- | ----------------- | ------------------------------- | -------------- | ---------------------------     |
| Logisitic         | Tjur's R2         | $\dagger$                       |                | `performace::r2_tjur()`         |
| Multinomial Logit | McFadden's R2     | $\ddagger$                      | 1 & 2          | `performace::r2_mcfadden()`     |
| GLM               | Nagelkerke's R2   | $\S$                            | 2              | `performace::r2_nagelkerke()`   |
| GLM               | Likelihood ratio  | Adjusted Nagelkerke - see below |                | `MuMIn::r2.squaredLR()`         |
| Mixed models      | Nakagawa's R2     | Too complex                     |                | `performace::r2_nakagawa()`     |
| Mixed models      |                   |                                 |                | `MuMIn::r.suaredGLMM()`         |
| ZI models         | Zero-inflated R2  | Too complex                     |                | `performace::r2_zeroinflated()` |
| Bayesian models   | Bayes R2          | Too complex                     |                | `performace::r2_bayes()`        |
 
$\dagger$: $R^2=\frac{1}{n_{1}}\sum \hat{\pi}(y=1) - \frac{1}{n_{0}}\sum \hat{\pi}(y=0)$

$\ddagger$: $R^2=1-\frac{logL(x)}{logL(0)}$

$\S$: $R^2=\frac{1-(\frac{logL(0)}{logL(x)})^{2/N}}{1-logl(0)^{2/N}}$

where $n_1$ and $n_0$ are the number of 1's and 0's in the response and
$\hat{\pi}$ is the predicted probability. $logL(x)$ and $logL(0)$ are the
log-likelihoods of the fitted and null models respectively and $N$ is the number of observations.

Note, if you run `performance::r2()`, the function will work out what
type of model has been fit and then use the appropriate function from
the above table.

## Deviance

$R^2 = \frac{unexplained variation}{total variation}$

For GLMS: 
$R^2 = \frac{unexplained deviance}{total deviance}$

Total amount unexlained comes from the null model.

Useing the formula above, the model explains 46%. R2 was calculated by the deviance.

```{r}
1 - (polis.glm$deviance/polis.glm$null)
```

## Tjur $R^2$

From the table: 

More complex than the above. Implies the model is explaining about 52%. R2 was calculated using Tjur (REF).

```{r Tjur R2}
polis.glm %>% performance::r2_tjur()
```

## Nagelkirk

Never use adjusted. Adjustments 'out there' for small sample size and elevates $R^2$. Take the smaller of the $R^2$ to be conservative.

$R^2$ = 0.47

```{r MuMIn}
polis.glm %>% MuMIn::r.squaredLR()
```

## LD50

In some disciplines it is useful to be able to calculate an LD50.  This is the value along the x-axis that corresponds to a probability of 50% - e.g. the switch-over point in Island perimeter to area Ratio at which the lizards go from more likely to be present to more likely to be absent.  It is the inflection point.

It is also the point at which the slope (when back-transformed) is at its
steepest and can be calculated as:

$$
-\frac{Intercept}{Slope}
$$

```{r calculate LD50}
polis.glm %>% coef()
polis.glm$coefficients
polis.glm$coef[1]
polis.glm$coef[2]
-polis.glm$coef[1]/polis.glm$coef[2]
```

LD50 (inflection point) corresponds to a RATIO of 16.4.

Only applies on the link scale. Don't back-transform.

Might want to know other LDs. Would also be nice to have CIs. 

There is a function to do this in MASS - old package in S.

Gives dose and SE - can calculate CI from SE. CI ~ 2 * SE ~ 95%

```{r other LDs, CIs}
ld <- polis.glm %>% MASS::dose.p(p = c(0.5, 0.9))
ld # hard to extract values

ld.SE <- attr(ld, 'SE') # extracting attributes
ld.SE

lds <- data.frame(LD = attr(ld, 'p'),
                  Dose = as.vector(ld),
                  SE = ld.SE) %>% 
  mutate(lower = Dose - SE * qnorm(0.975), # quantile of normal dist
         upper = Dose - SE * qnorm(0.975))

qnorm(0.975) # number of SE is 95% confidence interval
```

What shape island for RATIO 0.75?

```{r other LDs, CIs, with .75}
ld <- polis.glm %>% MASS::dose.p(p = c(0.5, 0.9, 0.75))
ld # hard to extract values

ld.SE <- attr(ld, 'SE') # extracting attributes
ld.SE

lds <- data.frame(LD = attr(ld, 'p'),
                  Dose = as.vector(ld),
                  SE = ld.SE) %>% 
  mutate(lower = Dose - SE * qnorm(0.975), # quantile of normal dist
         upper = Dose - SE * qnorm(0.975))
lds
```

For 90, lower is negative because of symmetric CI. +/- 2 SE of Dose. Turn negative to 0. Can't have negative ratio.

# Statistical methods section

The relationship between presence/absence of Uta lizards on the California Gulf Islands and the island perimeter to area ratio was explored using a logistic (implied in pres/abs) generalized linear model (logit-link)

$R^2$ values were calculated based on Tjur's method (REF).

All model and graphs were performed in R4.0.4 (R Core Team, 2021)

Assumed you will check model validation - can put in supplementary.

> Model checks were performed using DHARMa...

## Citing packages

Need to cite the statistical method which is sometimes the package documentation.

`performance::r2_tjur` for documentation just Reference listed.

GLMs been around so long don't need to reference. More complicated model should be referenced.

# Summary figures {.tabset .tabset-faded}

Useful summary figures to accompany statistical models typically
depict the modelled trend(s) overlayed onto the data used to fit the
model.  When there is only a single predictor, the data should be the
raw data.  However, when there are numerous predictors and the
modelled trend represents a trend associated with one (or more
predictors) marginalising over other predictor(s), then displaying the
raw data may not be satisfying.  The reason for this is that the
modelled _partial trend_ depicts the trend between the response and
the focal predictor(s) holding the other predictor(s) constant - that
is, standardising for the other predictor(s).  Plotting of raw data
will not reflect this standardisation.

Hence, rather than plot the raw data, we can instead plot the partial
observations (partial residuals).  To do so, add the (working)
residuals onto predictions associated with the observed predictor
values.

Unfortunately, this technique is not as straight forward for logistic
regression.  After adding the residuals to the predictions and
back-transforming to the response scale, it is necessary to transform
again to binomial.

In this case, since we just have a single predictor, we might as well
just add the raw data.

1. Grid of values between range of RATIO values. 

```{r make grid}
polis.grid <- with(polis, list(RATIO = seq_range(RATIO, n = 100)))
```

emmeans considers everything as categorical. So even though RATIO is continuous, only gives one value.

`at = polis.grid` gives predicted emmean for whole grid.

```{r emmean}
polis.glm %>% emmeans(~ RATIO) %>% # gives one thing
  head()
```

2. Predict across grid of ratio values.

```{r predict for polis grid}
polis.glm %>% emmeans(~ RATIO, at = polis.grid) %>% head() # values on logit scale
```

Provides CI asymp.LCL (lower), asymp.UCL (upper). Cannot calculate dfs. - complex issue. Mass disagreement as to how should be calculated.

Does not know dfs so assume Inf sample size. Do not rely on estimates of degrees of freedom that are hard to justify.

3. Back-transform to response scale.

```{r predict and transform back to response}
polis.glm %>% emmeans(~ RATIO, at = polis.grid, type = 'response') %>% 
  as.data.frame() -> newdata  # output as a dataframe to pot
newdata
```

```{r plot}
ggplot(newdata, aes(x = RATIO, y = prob)) +
  geom_ribbon(aes(ymin = asymp.LCL, ymax = asymp.UCL),
              fill = 'blue', alpha = 0.2) +
  geom_line() +
  geom_point(data = polis, aes(x = RATIO, y = PA), size = 2) +
  scale_y_continuous(expression(Presence~or~Absence)) +
  scale_x_continuous(expression(Island~Perimeter~to~Area~Ratio)) +
  theme_classic()
```

# References

