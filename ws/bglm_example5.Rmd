---
title: "Bayesian GLM Part5"
author: "Murray Logan"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: show
    collapse: no
    df_print: paged
    fig_caption: yes
    fig_height: 4
    fig_width: 4
    highlight: textmate
    theme: spacelab
    toc: yes
    toc_float: yes
    css: ../resources/style.css
  pdf_document:
    df_print: default
    fig_caption: yes
    fig_height: 4
    fig_width: 4
    highlight: tango
    latex_engine: xelatex
    number_sections: yes
    toc_depth: 2
  word_document:
    fig_caption: yes
    fig_height: 4
    fig_width: 4
    highlight: tango
    toc: yes
    toc_depth: 2
output_dir: "docs"
documentclass: article
fontsize: 12pt
mainfont: Arial
mathfont: LiberationMono
monofont: DejaVu Sans Mono
classoption: a4paper
bibliography: ../resources/references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```

# Preparations

Load the necessary libraries

```{r libraries, results='markdown', eval=TRUE}
library(rstanarm)   #for fitting models in STAN
library(brms)       #for fitting models in STAN
library(coda)       #for diagnostics
library(ggmcmc)     #for MCMC diagnostics
library(bayesplot)  #for diagnostics
library(rstan)      #for interfacing with STAN
library(DHARMa)     #for residual diagnostics
library(emmeans)    #for marginal means etc
library(broom)      #for tidying outputs
library(broom.mixed)
library(tidybayes)  #for more tidying outputs
library(ggeffects)  #for partial plots
library(tidyverse)  #for data wrangling etc
library(patchwork)
```


# Scenario

Here is a modified example from @Quinn-2002-2002. Day and Quinn
(1989) described an experiment that examined how rock surface type
affected the recruitment of barnacles to a rocky shore. The experiment
had a single factor, surface type, with 4 treatments or levels: algal
species 1 (ALG1), algal species 2 (ALG2), naturally bare surfaces (NB)
and artificially scraped bare surfaces (S). There were 5 replicate plots
for each surface type and the response (dependent) variable was the
number of newly recruited barnacles on each plot after 4 weeks.

![Six-plated barnacle](../resources/barnacles.jpg){width="224" height="308"}

Format of day.csv data files

TREAT   BARNACLE
------- ----------
ALG1    27
..      ..
ALG2    24
..      ..
NB      9
..      ..
S       12
..      ..

-------------- ----------------------------------------------------------------------------------------------------------------------------------------------
**TREAT**      Categorical listing of surface types. ALG1 = algal species 1, ALG2 = algal species 2, NB = naturally bare surface, S = scraped bare surface.
**BARNACLE**   The number of newly recruited barnacles on each plot after 4 weeks.
-------------- ----------------------------------------------------------------------------------------------------------------------------------------------



# Read in the data

```{r readData, results='markdown', eval=TRUE}
day = read_csv('../data/day.csv', trim_ws=TRUE)
glimpse(day)
```

Start by declaring the categorical variables as factor.

```{r prepare, results='markdown', eval=TRUE, hidden=TRUE}
day = day %>% mutate(TREAT=factor(TREAT))
```

Count data - poisson. Lambda only. Only estimate betas.

Log link function.

Model formula:
$$
\begin{align}
y_i &\sim{} \mathcal{Pois}(\lambda_i)\\
ln(\mu_i) &= \boldsymbol{\beta} \bf{X_i}\\
\beta_0 &\sim{} \mathcal{N}(0,10)\\
\beta_{1,2,3} &\sim{} \mathcal{N}(0,1)\\
\end{align}
$$

where $\boldsymbol{\beta}$ is a vector of effects parameters and $\bf{X}$ is a model matrix representing the intercept and treatment contrasts for the effects of Treatment on barnacle recruitment.

# Exploratory data analysis {.tabset .tabset-faded}

## Explor data for priors



```{r mean median }
day %>% group_by(TREAT) %>% summarize(log(median(BARNACLE)),
                                      log(mad(BARNACLE)))
mad(day$BARNACLE)
```

# Priors

$\beta_0$ estimated from above.
$\beta_i$ pretty standard. 

$$
\begin{align}
y_i &\sim{} \mathcal{Pois}(\lambda_i)\\
ln(\mu_i) &= \boldsymbol{\beta} \bf{X_i}\\
\beta_0 &\sim{} \mathcal{N}(3,2)\\
\beta_{1,2,3} &\sim{} \mathcal{N}(0,1)\\
\end{align}
$$

```{r set priors as above}
priors <- prior(normal(3,2), class = 'Intercept') +
  prior(normal(0,1), class = 'b')
```


# Fit the model {.tabset .tabset-faded}

## Prior only

Prior only to see what values we get from priors alone first.

```{r fit prior only model}
day.brm2 <- brm(bf(BARNACLE ~ TREAT,
                   family = poisson(link = 'log')),
                data = day,
                prior = priors,
                sample_prior = 'only',
                iter = 5000,
                warmup = 2500,
                chains = 3,
                thin = 5)
```

Suggest priors are not going to influence posteriors.

```{r check priors}
day.brm2 %>% conditional_effects() %>% plot(points = TRUE)
```

Priors will not be driving the posteriors.

Big points should be mean of the priors on log scale. Small dots are the data

```{r check priors w scaled y}
day.brm2 %>% conditional_effects() %>% plot(points = TRUE) %>% 
  lapply(function(x) x + scale_y_log10())
```

## Prior and data model

```{r prior and data}
day.brm3 <- day.brm2 %>% update(sample_prior = 'yes')
```

Influence of data and priors on the posterior. Big dots are the mean predictions and small dots are the data.

Credible interval is about a collection. What's the population like? 95% sure the population is within the bars from the model versus an individual prediction.

```{r check this model w scaled y}
day.brm3 %>% conditional_effects() %>% plot(points = TRUE) %>% 
  lapply(function(x) x + scale_y_log10())
```

```{r get variables}
day.brm3 %>% get_variables()
```

```{r check priors}
day.brm3 %>% hypothesis('TREATALG2 < 0') %>% plot()
```

# MCMC sampling diagnostics {.tabset .tabset-faded}

## Trace plots

```{r trace}
day.brm3$fit %>% stan_trace()
```

## Autocorrelation

```{r autocorr}
day.brm3$fit %>% stan_ac()
```

## Efficiency

```{r ess}
day.brm3$fit %>% stan_ess()
```

## Rhat

```{r rhat}
day.brm3$fit %>% stan_rhat()
```

## Spaghetti plot

Not fantastic or bad - small dataset. Flat top could be an artefact of that.

```{r spaghetti}
day.brm3 %>% pp_check(type = 'dens_overlay', ndraws = 250)
```

# Model validation {.tabset .tabset-faded}

## DHARMa residuals

Looks good.

Residuals, quantiles some have all above or below the line could be variance problem. Residual plot so patterns are bad.

```{r DHARMa}
preds <- day.brm3 %>% posterior_predict(nsamples = 250, summary = FALSE) #ndraws instead of nsample

day.resids <- createDHARMa(simulatedResponse = t(preds), 
                            observedResponse = day$BARNACLE, # only thing that will change
                            fittedPredictedResponse = apply(preds, 2, median), 
                            integerResponse = TRUE) # poisson need to tell it to make integer predictions
day.resids %>% plot()
```


# Partial effects plots {.tabset .tabset-faded}

Expectations:

- alg2 more barnacles than alg1
- algae more than bare and scrape

```{r partial plot}
day.brm3 %>% 
  ggemmeans(~TREAT) %>% 
  plot(add.data = TRUE, jitter = c(0.5, 0)) # only jitter on the x
```


# Model investigation {.tabset .tabset-faded}

Intercept 3.10 on a log scale -> 22

ALG2 0.24 -> 1.28 x more than ALG1 on a fold scale.

```{r summary}
day.brm3 %>% summary() # useful in a table, maybe back transformed

exp(3.1)
exp(0.24) # 1.28 x more than ALG1 on a fold scale

exp(3.1 + 0.25) # number in alg2
exp(3.1 + 0.25) - exp(3.09) # ~6 more in alg2
```

Calculate ourselves and have full control.

```{r get var2}
day.brm3 %>% get_variables()
```

Using gather draws need to nominate which variables we want.

```{r gather dras}
day.draw <- day.brm3 %>% 
  gather_draws(`^b_.*`, regex = TRUE) # back ticks
day.draw
```

Stuff on fold/factor scale (is a ratio of that:this)

Nicer table to use. 

1.27x more in alg2 than alg1.

0.68x recruits in NB compared to ALG1. 0.6x recruits in X than ALG1.

```{r factor scale table}
day.draw %>% mutate(Exp = exp(.value)) %>% median_hdci(Exp)
```

Probability barnacles in ALG2 in ALG1. Scale does not matter for testing hypothesis - the cut off of comparison changes though.

How much evidence for this change?

`P1 = sum(Exp > 1)` on fold scale so > 1. Using .value would be > 0.

Strong evidence more barnacles on ALG2 than ALG1.

A lot of evidence that ALG1 has more recruits than naturally bare.

```{r p-values}
day.draw %>% mutate(Exp = exp(.value)) %>% summarize(P1 = sum(Exp > 1)/n(), # positive
                                                     P2 = sum(Exp < 1)/n()) # negative
```

# Further investigations 

Bayesian can do whatever contrasts - no statistical constraints. Don't even have to be independent.

Posterior does not change so one test does not have any bearing on another test.

## Post-hoc test (Tukey's){.tabset .tabset-faded}

### Standard Tukey's test

ratio - ALG1 has 1.48x more recruits than NB.

Have some of the same tests ALG1/ALG2, but expressed opposite terms. 1.22 vs 0.78.

Any evidence that ALG2 and NB are different? Any evidence that more on NB than S?

NB/S includes 1 so no evidence. But not quantifying the evicence.

```{r std tukeys}
day.brm3 %>% emmeans(~TREAT, type = 'response') %>% 
  pairs()
```

Strong evidence more on AGL2 than NB.

Weak evidence more on NB than S.

> < 0.8 is weak

```{r gather draws}
day.brm3 %>% emmeans(~TREAT, type = 'link') %>% 
  pairs() %>% 
  gather_emmeans_draws() %>% # gather the draws
  mutate(Fit = exp(.value)) -> day.em # back transformed here to factor scale

day.em %>% summarize(P = sum(Fit > 1)/n(), P2 = sum(Fit < 1)/n()) # 1 for factor
```

No evidence 20% difference between ALG1-ALG2.

Strong evidence (92% confidence) that ALG1 has 20% more recruits than NB.

Not inverse because ... certain conditions don't apply. <10%?

```{r}
day.em %>% summarize(P1 = sum(Fit > 1.2)/n(), P2 = sum(Fit < 0.8)/n()) # 1/1.2 is 0.8
```


## Planned contrasts{.tabset .tabset-faded}

Define your own

Compare:

a) ALG1 vs ALG2
b) NB vs S
c) average of ALG1+ALG2 vs NB+S - limit of frequentist
d) average of ALG1+ALG2 to NB

Covariance matrix. Must know the levels of your categories. The order is important.

Frequentist - have already violated 2 assumptions. No independnet, too many comparisons.

```{r cmat}
cmat <- cbind('Alg2_Alg1' = c(-1, 1, 0, 0), # order of levels ALG1 ALG2 NB S
              'NB_S' = c(0, 0, 1, -1),
              'Alg_Bare' = c(0.5, 0.5, -0.5, -0.5),
              'Alg_NB' = c(0.5, 0.5, -1, 0))
```

Already seen ~30% more from ALG2-ALG1. Didn't see last 2.

77% > recruitment in ave ALGE vs Bare.
66% > recruitment in ave ALG vs naturally bare.

```{r fit contrasts}
day.brm3 %>% 
  emmeans(~TREAT, type = 'response') %>% 
  contrast(method = list(TREAT = cmat))
```

Same code as before but replacing `pairs()` with `contrast()`.

Strong evidence more recruits on algal surface than bare.

```{r probability contrasts}
day.brm3 %>% 
  emmeans(~TREAT, type = 'link') %>% 
  contrast(method = list(TREAT = cmat)) %>% 
  gather_emmeans_draws() %>%  # bug seems to forget we want on response so just do link
  mutate(Fit = exp(.value)) -> day.em

day.em %>% 
  summarize(P = sum(Fit > 1)/n(), P2 = sum(Fit < 1)/n())
```

# Summary Figure {.tabset .tabset-faded}

Xintercept at 1.5 denotes 50% increase.

Strong evidence that ALG2 > ALG1, but week evidence that it is 50% more.

Fold scale is nice for ecological data. X% percent more vs Y individuals more.

```{r summary on fold scale}
day.em %>% 
  ggplot() +
  geom_vline(xintercept = 1, linetype = 'dashed') +
  geom_vline(xintercept = 1.5, alpha = 0.3, linetype = 'dashed') +
  stat_slab(aes(x = Fit, y = contrast,
                fill = stat(ggdist::cut_cdf_qi(cdf,
                                               .width = c(0.5, 0.8, 0.95),
                                               labels = scales::percent_format())
                            )), color = 'black') +
  scale_fill_brewer('Interval', direction = -1, na.translate = FALSE) +
  scale_x_continuous(trans = scales::log2_trans(), breaks = c(0.5, 0.8, 1, 1.2, 1.5, 2, 4)) +
  theme_classic()
```



```{r fitModel, results='markdown', echo=FALSE,eval=FALSE, hidden=TRUE}
day.rstanarm <- stan_glm(BARNACLE ~ TREAT, data=day,
                      family=poisson(link='log'),
                      chains = 3,iter = 5000, warmup=2000, thin=5,
                      refresh=0)
prior_summary(day.rstanarm)

day.rstanarm <- stan_glm(BARNACLE ~ TREAT, data=day, family='poisson',
                      prior = normal(c(0,0,0), c(2.5,2.5,2.5)),
                      prior_intercept = normal(0,10),
                      chains = 3,iter = 2000, thin=2, refresh=0)
prior_summary(day.rstanarm)

plot(day.rstanarm,  'mcmc_trace')
plot(day.rstanarm,  'mcmc_acf_bar')
plot(day.rstanarm,  'mcmc_rhat_hist')
plot(day.rstanarm,  'mcmc_neff_hist')


preds <- posterior_predict(day.rstanarm,  nsamples=250,  summary=FALSE)
day.resids <- createDHARMa(simulatedResponse = t(preds),
                            observedResponse = day$BARNACLE,
                           fittedPredictedResponse = apply(preds, 2, median),
                           integerResponse = TRUE)
plot(day.resids)


#pp_check(day.rstanarm, x=as.numeric(day$TREAT),'intervals')




## Compare the proportion of zeros in the observed and expected data
#yrep = posterior_predict(day.rstan)
prop_zero <- function(y) mean(y == 0)
(prop_zero_test1 <- pp_check(day.rstanarm, plotfun = "stat", stat = "prop_zero"))
                                        # no zeros - so not zero inflated


day.rstanarmNB <- stan_glm(BARNACLE ~ TREAT, data=day,
                      family='neg_binomial_2',
                      chains = 3,iter = 5000, thin=5, warmup=2000, refresh=0)
preds <- posterior_predict(day.rstanarmNB,  nsamples=250,  summary=FALSE)
day.resids <- createDHARMa(simulatedResponse = t(preds),
                            observedResponse = day$BARNACLE,
                            fittedPredictedResponse = apply(preds, 2, median))
plot(day.resids)


day.rstanNB <- update(day.rstan, family = neg_binomial_2)
(loo.P=loo(day.rstan))
(loo.NB=loo(day.rstanNB))
compare_models(loo.P, loo.NB)

ggpredict(day.rstanarm, term='TREAT') %>% plot
ggpredict(day.rstanarm, ~TREAT) %>% plot
ggemmeans(day.rstanarm, ~TREAT) %>% plot


summary(day.rstanarm)
library(tidybayes)
tidyMCMC(day.rstanarm$stanfit, conf.int=TRUE,
         conf.method='HPDinterval', rhat=TRUE,ess=TRUE)


# Pairwise comparisons
library(emmeans)
## factor statements
emmeans(day.rstanarm, pairwise~TREAT, type='response')
## what about probabilities
day.em = emmeans(day.rstanarm, pairwise~TREAT, type='link')$contrasts %>%
    gather_emmeans_draws() %>%
    mutate(Fit=exp(.value))
day.em %>% head
day.em %>% group_by(contrast) %>%
    ggplot(aes(x=Fit)) +
    geom_histogram() +
    geom_vline(xintercept=1, color='red') + 
    facet_wrap(~contrast, scales='free')
day.em %>% group_by(contrast) %>% median_hdi()
# Probability of effect
day.em %>% group_by(contrast) %>% summarize(P=sum(Fit>1)/n())
##Probability of effect greater than 10%
day.em %>% group_by(contrast) %>% summarize(P=sum(Fit>1.1)/n())


##Planned contrasts
cmat<-cbind('Alg2_Alg1'=c(-1,1,0,0),
              'NB_S'=c(0,0,1,-1),
             'Alg_Bare'=c(0.5,0.5,-0.5,-0.5),
             'Alg_NB'=c(0.5,0.5,-1,0))
#crossprod(cmat)
emmeans(day.rstanarm, ~TREAT, contr=list(TREAT=cmat), type='link')
emmeans(day.rstanarm, ~TREAT, contr=list(TREAT=cmat), type='response')
day.em = emmeans(day.rstanarm, ~TREAT, contr=list(TREAT=cmat), type='link')$contrasts %>%
      gather_emmeans_draws() %>% mutate(Fit=exp(.value)) 
day.em %>% group_by(contrast) %>% mean_hdi()
# Probability of effect
day.em %>% group_by(contrast) %>% summarize(P=sum(Fit>1)/n())
##Probability of effect greater than 10%
day.em %>% group_by(contrast) %>% summarize(P=sum(Fit>1.5)/n())

hist(bayes_R2(day.rstanarmNB))

bayes_R2(day.rstanarm) %>% median_hdi
bayes_R2(day.rstanarmNB) %>% hist


## Summary plot
day.grid = with(day, list(TREAT=levels(TREAT)))
newdata = emmeans(day.rstanarm, ~TREAT, type='response') %>% as.data.frame
head(newdata)
ggplot(newdata, aes(y=rate, x=TREAT)) +
    geom_pointrange(aes(ymin=lower.HPD, ymax=upper.HPD))
```

                                           
```{r fitModel.brms, results='markdown', eval=FALSE, hidden=TRUE}
day.form <- bf(BARNACLE ~ TREAT,  family=poisson(link='log'))
get_prior(day.form,  data=day)
day.priors <- c(
  prior(normal(0, 10),  class='Intercept'),
  prior(normal(0, 2.5), class='b')
)
day.brms <- brm(day.form, data=day,
                prior=day.priors, 
                 chains=3,  iter=5000,  warmup=2000, thin=5,
                 refresh=0)

plot(day.brms)
mcmc_plot(day.brms,  type='acf_bar')
mcmc_plot(day.brms,  type='rhat_hist')
mcmc_plot(day.brms,  type='neff_hist')

preds <- posterior_predict(day.brms,  nsamples=250,  summary=FALSE)
day.resids <- createDHARMa(simulatedResponse = t(preds),
                            observedResponse = day$BARNACLE,
                           fittedPredictedResponse = apply(preds, 2, median),
                           integerResponse = TRUE)
plot(day.resids)

                                        #pp_check(day.brmsP, x=as.numeric(day$TREAT),'intervals')


ggpredict(day.brms, term='TREAT') %>% plot
ggpredict(day.brms, ~TREAT) %>% plot
ggemmeans(day.brms, ~TREAT) %>% plot

summary(day.brms)

tidyMCMC(day.brms$fit, conf.int=TRUE,
         conf.method='HPDinterval', rhat=TRUE,ess=TRUE)

# Pairwise comparisons
library(emmeans)
## factor statements
emmeans(day.brms, pairwise~TREAT, type='response')
## what about probabilities
day.em = emmeans(day.brms, pairwise~TREAT, type='link')$contrasts %>%
    gather_emmeans_draws() %>%
    mutate(Fit=exp(.value))
day.em %>% head
day.em %>% group_by(contrast) %>%
    ggplot(aes(x=Fit)) +
    geom_histogram() +
    geom_vline(xintercept=1, color='red') + 
    facet_wrap(~contrast, scales='free')
day.em %>% group_by(contrast) %>% median_hdi(.width=c(0.8, 0.95))

day.sum <- day.em %>%
  group_by(contrast) %>%
  median_hdci(.width=c(0.8, 0.95))
day.sum
ggplot(day.sum) +
  geom_hline(yintercept=1, linetype='dashed') +
  geom_pointrange(aes(x=contrast, y=Fit, ymin=Fit.lower, ymax=Fit.upper, size=factor(.width)),
                  show.legend = FALSE) +
  scale_size_manual(values=c(1, 0.5)) +
  coord_flip()

g1 <- ggplot(day.sum) +
  geom_hline(yintercept=1) +
  geom_pointrange(aes(x=contrast, y=Fit, ymin=Fit.lower, ymax=Fit.upper, size=factor(.width)), show.legend = FALSE) +
  scale_size_manual(values=c(1, 0.5)) +
  scale_y_continuous(trans=scales::log2_trans(),  breaks=c(0.5, 1, 2, 4)) +
  coord_flip()
g1
                                        # Probability of effect
day.em %>% group_by(contrast) %>% summarize(P=sum(.value>0)/n())
day.em %>% group_by(contrast) %>% summarize(P=sum(Fit>1)/n())
##Probability of effect greater than 10%
day.em %>% group_by(contrast) %>% summarize(P=sum(Fit>1.1)/n())

##Planned contrasts
cmat<-cbind('Alg2_Alg1'=c(-1,1,0,0),
              'NB_S'=c(0,0,1,-1),
             'Alg_Bare'=c(0.5,0.5,-0.5,-0.5),
             'Alg_NB'=c(0.5,0.5,-1,0))
#crossprod(cmat)
emmeans(day.brms, ~TREAT, contr=list(TREAT=cmat), type='link')
emmeans(day.brms, ~TREAT, contr=list(TREAT=cmat), type='response')
day.em = emmeans(day.brms, ~TREAT, contr=list(TREAT=cmat), type='link')$contrasts %>%
      gather_emmeans_draws() %>%
      mutate(Fit=exp(.value)) 
day.em %>% group_by(contrast) %>% median_hdci()
# Probability of effect
day.em %>% group_by(contrast) %>% summarize(P=sum(Fit>1)/n())
##Probability of effect greater than 10%
day.em %>% group_by(contrast) %>% summarize(P=sum(Fit>1.1)/n())

hist(bayes_R2(day.brms, summary=FALSE))

bayes_R2(day.brms, summary=FALSE) %>% median_hdi


## Summary plot
day.grid = with(day, list(TREAT=levels(TREAT)))
newdata = emmeans(day.brms, ~TREAT, type='response') %>% as.data.frame
head(newdata)
g2 <- ggplot(newdata, aes(y=rate, x=TREAT)) +
  geom_pointrange(aes(ymin=lower.HPD, ymax=upper.HPD))

library(patchwork)
g1 + g2
```

# References
